{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random \n",
    "import string\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename,expected_bytes):\n",
    "    \n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)   \n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "                   'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 1000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950 d against early working class radicals including the diggers of \n",
      "50  anarchism originated as a term of abuse first use\n"
     ]
    }
   ],
   "source": [
    "valid_size = 100\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:60])\n",
    "print(valid_size, valid_text[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d agains', 'cribe an', 'reek wit', 'at this ', 'does not']\n",
      "--------------------------------------------------------------------------------\n",
      "[' an']\n",
      "['nar']\n"
     ]
    }
   ],
   "source": [
    "batch_size=5\n",
    "num_unrollings=7\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print('-'*80)\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM WITH BIGRAMS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One layer LSTM model\n",
    "num_nodes=90\n",
    "embedding_size= 128\n",
    "\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "    #Input data\n",
    "    train_data=list()\n",
    "    for _ in range(num_unrollings+1):\n",
    "        train_data.append(\n",
    "                         tf.placeholder(tf.float32,shape=[batch_size,vocabulary_size]))\n",
    "    \n",
    "    train_chars=train_data[:num_unrollings]\n",
    "    train_inputs=zip(train_chars[:-1], train_chars[1:])\n",
    "    train_labels=train_data[2:]\n",
    "        \n",
    " \n",
    "    vocabulary_embeddings=tf.Variable(tf.random_uniform([vocabulary_size*vocabulary_size,embedding_size],\n",
    "                                                       -1.0,1.0))\n",
    "    \n",
    "        \n",
    "    #cell parameters\n",
    "    #for inputgate\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size,num_nodes],-0.1,0.1))\n",
    "    ih=tf.Variable(tf.truncated_normal([num_nodes,num_nodes],-0.1,0.1))\n",
    "    ib=tf.Variable(tf.zeros([1,num_nodes]))\n",
    "    \n",
    "    #for input state\n",
    "    gx=tf.Variable(tf.truncated_normal([embedding_size,num_nodes],-0.1,0.1))\n",
    "    gh=tf.Variable(tf.truncated_normal([num_nodes,num_nodes],-0.1,0.1))\n",
    "    gb=tf.Variable(tf.zeros([1,num_nodes]))\n",
    "    \n",
    "    #for forget Gate\n",
    "    fx=tf.Variable(tf.truncated_normal([embedding_size,num_nodes],-0.1,0.1))\n",
    "    fh=tf.Variable(tf.truncated_normal([num_nodes,num_nodes],-0.1,0.1))\n",
    "    fb=tf.Variable(tf.zeros([1,num_nodes]))\n",
    "    \n",
    "    #for output gate\n",
    "    ox=tf.Variable(tf.truncated_normal([embedding_size,num_nodes],-0.1,0.1))\n",
    "    oh=tf.Variable(tf.truncated_normal([num_nodes,num_nodes],-0.1,0.1))\n",
    "    ob=tf.Variable(tf.zeros([1,num_nodes]))\n",
    "   \n",
    "    #classifier parameteres\n",
    "    w=tf.Variable(tf.truncated_normal([num_nodes,vocabulary_size],-0.1,0.1))\n",
    "    b=tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    '''note that this LSTM architechture solves vanishing gradient problem via its amazing inner \n",
    "      properties of propagating error backward smoothly between many time steps.'''\n",
    "    \n",
    "    #lstm cell computation\n",
    "    def lstm_cell(input,prev_h,prev_s):\n",
    "        g=tf.tanh( tf.matmul(input,gx) + tf.matmul(prev_h,gh) + gb )\n",
    "        input_gate=tf.sigmoid( tf.matmul(input,ix) + tf.matmul(prev_h,ih) + ib )\n",
    "        \n",
    "        forget_gate=tf.sigmoid( tf.matmul(input,fx) + tf.matmul(prev_h,fh) + fb )\n",
    "        \n",
    "        state= g*input_gate + forget_gate*prev_s\n",
    "        \n",
    "        output_gate=tf.sigmoid( tf.matmul(input,ox) + tf.matmul(prev_h,oh) + ob )\n",
    "        \n",
    "        output = tf.tanh(state) * output_gate\n",
    "        return output,state\n",
    "    \n",
    "        \n",
    "    #for saving states and output during unrolling.\n",
    "    saved_value_output=tf.Variable(tf.zeros([batch_size,num_nodes]),trainable=False)\n",
    "    saved_value_state=tf.Variable(tf.zeros([batch_size,num_nodes]),trainable=False)\n",
    "     \n",
    "\n",
    "        \n",
    "    #unrolling LSTM\n",
    "    outputs=list()  #for storing hiddenvalues(output) of lstm cell\n",
    "    output=saved_value_output\n",
    "    state=saved_value_state\n",
    "    \n",
    "    for i in train_inputs:\n",
    "        bigram_index= tf.arg_max(i[0],dimension=1) + vocabulary_size* tf.arg_max(i[1],dimension=1)\n",
    "        iembed=tf.nn.embedding_lookup(vocabulary_embeddings,bigram_index)\n",
    "        \n",
    "        '''dropout applied in LSTM only in depth(non recurrent connections)'''\n",
    "        \n",
    "        dropout_i=tf.nn.dropout(iembed,0.5)  #dropout in input\n",
    "        \n",
    "        output,state = lstm_cell(dropout_i,output,state)\n",
    "        \n",
    "        drop_output=tf.nn.dropout(output,0.5)   #dropout in depth output\n",
    "        outputs.append(drop_output)\n",
    "      \n",
    "        \n",
    "        \n",
    "     #to save output and state so next sequeces can use it. \n",
    "    with tf.control_dependencies([saved_value_output.assign(output),\n",
    "                                  saved_value_state.assign(state)]):\n",
    "        \n",
    "        logits=tf.nn.xw_plus_b(tf.concat(0,outputs),w,b)\n",
    "        \n",
    "        #Regularization\n",
    "        regularizer=tf.nn.l2_loss(w) + tf.nn.l2_loss(b)\n",
    "        \n",
    "        loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits,\n",
    "                                                             tf.concat(0,train_labels)))\n",
    "        loss+=5e-4*regularizer\n",
    "        \n",
    "    #Optimizer\n",
    "    global_step=tf.Variable(0,trainable=False)\n",
    "    starter_lr=10.0\n",
    "    learning_rate=tf.train.exponential_decay(starter_lr,global_step,5000,0.1,\n",
    "                                                staircase=True)\n",
    "        \n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients,v = zip(*optimizer.compute_gradients(loss))\n",
    "        \n",
    "    #to handle exploding gradient problem.\n",
    "    gradients,_=tf.clip_by_global_norm(gradients,1.30)\n",
    "        \n",
    "    optimizer=optimizer.apply_gradients(zip(gradients,v),global_step=global_step)\n",
    "        \n",
    "        \n",
    "    #for predictions\n",
    "    train_preds=tf.nn.softmax(logits)\n",
    "    \n",
    "    #sample predictions\n",
    "    \n",
    "    sample_input=list()\n",
    "    for i in range(2):\n",
    "        sample_input.append(tf.placeholder(tf.float32,shape=[1,vocabulary_size]) )\n",
    "        \n",
    "    sample_biindex=tf.arg_max(sample_input[0],dimension=1) + vocabulary_size* tf.arg_max(sample_input[1],\n",
    "                                                                                        dimension=1)\n",
    "    \n",
    "    sample_embed=tf.nn.embedding_lookup(vocabulary_embeddings,sample_biindex)\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    reset_sample_state = tf.group(\n",
    "                   saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "                         saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    \n",
    "    sample_output,sample_state=lstm_cell(sample_embed,saved_sample_output,saved_sample_state)\n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        valid_preds=tf.nn.softmax(tf.nn.xw_plus_b(sample_output,w,b))\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 0: 3.305193 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.06\n",
      "====================================================================================================\n",
      "aslr hnvhregtmxruztovtnxaaptn i  ayiatllktc  mppdoarph i ayyjw wludtcyt ttptdtgi \n",
      "fkr xwat ah ztd n ntojaubtnanv  ju pqtiumgturvcmaoryekejhd lr pvoa guvpaeas xtac \n",
      "hwa lcnt ama ull somos uata or ntqcah ap lr lvta   jyutyaolauksgtomktkah s oje it\n",
      "x h ttxtw qttqzsnhztjaat ttdtvit ayjc  ntr ctaorwxll p  aratp t  x  hbnyn ntsmt  \n",
      "gkbht    o ma ftge zqpqtadhtmp  c  paeqtard rmkta wathjpmk ao eyc jg t i t ctdte \n",
      "gitndter axstib i hlq po sr g  lc ttqrl skrbra vwteovtssfpmba  etsas e ctz n wsg \n",
      "aleqq bq ab d rs aztaaquuatrta aa yreltr qtq watsmtjuij dtqsxwtodtvk y aatlsagtca\n",
      "================================================================================\n",
      "Validation set perplexity: 40.74\n",
      "Average loss at step 100: 3.277808 learning rate: 10.000000\n",
      "Minibatch perplexity: 17.58\n",
      "Validation set perplexity: 27.95\n",
      "Average loss at step 200: 3.140219 learning rate: 10.000000\n",
      "Minibatch perplexity: 15.48\n",
      "Validation set perplexity: 11.95\n",
      "Average loss at step 300: 3.033390 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.51\n",
      "Validation set perplexity: 11.76\n",
      "Average loss at step 400: 2.999719 learning rate: 10.000000\n",
      "Minibatch perplexity: 15.83\n",
      "Validation set perplexity: 11.92\n",
      "Average loss at step 500: 2.878055 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.13\n",
      "Validation set perplexity: 14.43\n",
      "Average loss at step 600: 2.932324 learning rate: 10.000000\n",
      "Minibatch perplexity: 15.52\n",
      "Validation set perplexity: 14.98\n",
      "Average loss at step 700: 2.896460 learning rate: 10.000000\n",
      "Minibatch perplexity: 18.14\n",
      "Validation set perplexity: 13.77\n",
      "Average loss at step 800: 2.918838 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.49\n",
      "Validation set perplexity: 17.18\n",
      "Average loss at step 900: 2.867086 learning rate: 10.000000\n",
      "Minibatch perplexity: 14.99\n",
      "Validation set perplexity: 11.57\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import collections\n",
    "num_steps = 100000\n",
    "summary_frequency = 1000\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "with tf.Session(graph=graph) as se:\n",
    "    tf.initialize_all_variables().run()\n",
    "    \n",
    "    mean_loss=0\n",
    "    for step in range(num_steps):\n",
    "        batche=train_batches.next()\n",
    "        feed_dict=dict()\n",
    "        for i in range(num_unrollings+1):\n",
    "            feed_dict[train_data[i]]=batche[i]\n",
    "        \n",
    "        _,l,lr,predictions = se.run([optimizer,loss,learning_rate,train_preds],feed_dict=feed_dict)\n",
    "        \n",
    "        mean_loss+=l\n",
    "        if step% summary_frequency ==0:\n",
    "            \n",
    "            if step>0:\n",
    "                mean_loss=mean_loss/summary_frequency\n",
    "                \n",
    "            print(\n",
    "                        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0    \n",
    "            \n",
    "            labels = np.concatenate(list(batche)[2:])\n",
    "            \n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                   np.exp(logprob(predictions, labels))))\n",
    "            \n",
    "            if step% (summary_frequency*10) ==0:\n",
    "                \n",
    "                #to generate some sample sentences\n",
    "                print('='*100)\n",
    "                for _ in range(7):\n",
    "                    feed = collections.deque(maxlen=2)\n",
    "                    for _ in range(2):  \n",
    "                        feed.append(random_distribution())\n",
    "                        \n",
    "                    sentence = characters(feed[0])[0] + characters(feed[1])[0]    \n",
    "                    reset_sample_state.run()\n",
    "                    \n",
    "                    for _ in range(79):\n",
    "                        prediction = valid_preds.eval({\n",
    "                                   sample_input[0]: feed[0],\n",
    "                                   sample_input[1]: feed[1]\n",
    "                                                           })\n",
    "                        feed.append(sample(prediction))\n",
    "                        sentence += characters(feed[1])[0]\n",
    "                        \n",
    "                    print(sentence)    \n",
    "                print('=' * 80)  \n",
    "            \n",
    "            #validation set perplexity\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for i in range(valid_size):\n",
    "                b=valid_batches.next()\n",
    "                predictions = valid_preds.eval({\n",
    "                                sample_input[0]: b[0],\n",
    "                                sample_input[1]: b[1]\n",
    "                                        })\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                                   valid_logprob / valid_size)))    \n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
